<!DOCTYPE html>
<html><head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>SANeRF-HQ</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <meta property="og:image" content="https://dorverbin.github.io/refnerf/img/refnerf_titlecard.jpg">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://dorverbin.github.io/refnerf">
    <meta property="og:title" content="Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields">
    <meta property="og:description" content="Neural Radiance Fields (NeRF) is a popular view synthesis technique that represents a scene as a continuous volumetric function, parameterized by multilayer perceptrons that provide the volume density and view-dependent emitted radiance at each location. While NeRF-based techniques excel at representing fine geometric structures with smoothly varying view-dependent appearance, they often fail to accurately capture and reproduce the appearance of glossy surfaces. We address this limitation by introducing Ref-NeRF, which replaces NeRF's parameterization of view-dependent outgoing radiance with a representation of reflected radiance and structures this function using a collection of spatially-varying scene properties. We show that together with a regularizer on normal vectors, our model significantly improves the realism and accuracy of specular reflections. Furthermore, we show that our model's internal representation of outgoing radiance is interpretable and useful for scene editing.">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields">
    <meta name="twitter:description" content="Neural Radiance Fields (NeRF) is a popular view synthesis technique that represents a scene as a continuous volumetric function, parameterized by multilayer perceptrons that provide the volume density and view-dependent emitted radiance at each location. While NeRF-based techniques excel at representing fine geometric structures with smoothly varying view-dependent appearance, they often fail to accurately capture and reproduce the appearance of glossy surfaces. We address this limitation by introducing Ref-NeRF, which replaces NeRF's parameterization of view-dependent outgoing radiance with a representation of reflected radiance and structures this function using a collection of spatially-varying scene properties. We show that together with a regularizer on normal vectors, our model significantly improves the realism and accuracy of specular reflections. Furthermore, we show that our model's internal representation of outgoing radiance is interpretable and useful for scene editing.">
    <meta name="twitter:image" content="https://dorverbin.github.io/refnerf/img/refnerf_titlecard.jpg"> -->


    <!-- mirror: F0%9F%AA%9E&lt -->
    <!-- <link rel="icon" href="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text y=%22.9em%22 font-size=%2290%22&gt;%E2%9C%A8&lt;/text&gt;&lt;/svg&gt;"> -->
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/font-awesome.min.css">
    <link rel="stylesheet" href="css/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <script src="js/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/codemirror.min.js"></script>
    <script src="js/clipboard.min.js"></script>
    <script src="js/video_comparison.js"></script>
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="header" style="text-align: center; margin: auto;">
        <div class="row" id="title-row" style="max-width: 100%; margin: 0 auto; display: inline-block">
            <h2 class="col-md-12 text-center" id="title">
                <b>SANeRF-HQ</b>: Segment Anything for NeRF in High Quality <br>
                <!-- <small>
                    
                </small> -->
            </h2>
        </div>
        <div class="row" id="author-row" style="margin:0 auto;">
            <div class="col-md-12 text-center" style="display: table; margin:0 auto">
                <table class="author-table" id="author-table">
                    <tr>
                        <td>
                            <a style="text-decoration:none" href="https://lyclyc52.github.io/">
                              Yichen Liu
                            </a>
                            <br>The Hong Kong University of Science and Technology
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://zymk9.github.io/">
                              Benran Hu
                            </a>
                            <br>Carnegie Mellon University
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://cse.hkust.edu.hk/~cktang/">
                             Chi-Keung Tang
                            </a>
                            <br>The Hong Kong University of Science and Technology
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://yuwingtai.github.io/">
                                Yu-Wing Tai
                            </a>
                            <br>Dartmouth College
                        </td>
                    </tr>
                </table>
            </div>
        </div>
    </div>
    <script>
        document.getElementById('author-row').style.maxWidth = document.getElementById("title-row").clientWidth + 'px';
    </script>
    <div class="container" id="main">
        <div class="row">
                <div class="col-sm-6 col-sm-offset-3 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="http://arxiv.org/abs/2312.01531">
                            <img src="./img/paper_image.jpg" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <!-- <li>
                            <a href="https://youtu.be/qrdRH9irAlk">
                            <img src="./img/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://storage.googleapis.com/gresearch/refraw360/ref.zip" target="_blank">
                            <image src="img/database_icon.png" height="60px">
                                <h4><strong>Shiny Dataset</strong></h4>
                            </a>
                        </li>
                        <li>  
                            <a href="https://dorverbin.github.io/refnerf/data/shiny_blender_source.zip" target="_blank">
                            <image src="img/database_icon.png" height="60px">
                                <h4><strong>Shiny Dataset Source</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://storage.googleapis.com/gresearch/refraw360/ref_real.zip" target="_blank">
                            <image src="img/real_database_icon.png" height="60px">
                                <h4><strong>Real Dataset</strong></h4>
                            </a>
                        </li>                             -->
                        <li>
                            <a href="https://github.com/lyclyc52/SANeRF-HQ" target="_blank">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>



        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <div class="video-compare-container" id="materialsDiv">
                    <video class="video" id="materials" loop playsinline autoPlay muted src="video/materials_circle_mipnerf_ours.mp4" onplay="resizeAndPlay(this)"></video>
                    
                    <canvas height=0 class="videoMerge" id="materialsMerge"></canvas>
                </div>
			</div>
        </div> -->


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Recently, the Segment Anything Model (SAM) has showcased remarkable capabilities of zero-shot segmentation, while  NeRF (Neural Radiance Fields) has gained popularity as a method for various 3D problems beyond novel view synthesis. Though there exist initial attempts to incorporate these two methods into 3D segmentation, they face the challenge of accurately and consistently segmenting objects in complex scenarios. In this paper, we introduce the Segment Anything for NeRF in High Quality (SANeRF-HQ) to achieve high quality 3D segmentation of any object in a given scene. SANeRF-HQ utilizes SAM for open-world object segmentation guided by user-supplied prompts, while leveraging NeRF to aggregate information from different viewpoints. To overcome the aforementioned challenges, we employ density field and RGB similarity to enhance the accuracy of segmentation boundary during the aggregation. Emphasizing on segmentation accuracy, we evaluate our method quantitatively on multiple NeRF datasets where high-quality ground-truths are available or manually annotated. SANeRF-HQ shows a significant quality improvement over previous state-of-the-art methods in NeRF object segmentation, provides higher flexibility for object localization, and enables more consistent object segmentation across multiple views.                
                </p>
            </div>
        </div>

        <image src="img/arch.png" class="img-responsive" alt="overview" width="60%" style="max-height: 450px;margin:auto;">

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <!-- <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://youtube.com/embed/qrdRH9irAlk" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div> -->
                    <video id="demo" width="100%" playsinline controls muted>
                        <source src="video/demo.mp4" type="video/mp4" />
                    </video>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Mask Aggregation
                </h3>
                <div class="text-justify">
                    Previous works directly decode masks from distilled SAM features, or use self-prompting to iteratively inverse render SAM masks. 
                    The former suffers from aliasing in the rendered SAM features and cannot produce consistent masks across views, while the latter may accumulate errors due to incorrect self-prompting.
                    Hence, we propose to aggregate the imperfect SAM 2D masks in the 3D space to generate high-quality and consistent 3D masks.                    
                    <br><br>
                    
                </div>
                <div class="text-center">
                    <!-- <video id="refdir" width="40%" playsinline autoplay loop muted>
                        <source src="video/reflection_animation.mp4" type="video/mp4" />
                    </video> -->
                    <img src="./img/comparison_1.png" width="100%">
                    <img src="./img/comparison_2.png" width="100%">
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Ray-Pair RGB Loss
                </h3>
                <div class="text-justify">
                    Segmentation errors in both 3D and 2D are more likely to occur at object boundaries. One observation is that humans usually distinguish object boundaries by the color and texture difference on the two sides. Here we introduce the Ray-Pair RGB loss, aiming to incorporate color and spatial information to improve the segmentation quality.                
                </div>
                <br>
                <div class="text-center">
                    <img src="./img/ray-pair_rgb_loss.png" width="50%">
                    <img src="./img/sim.png" width="80%">
                </div>
                <br>
                <div class="text-justify">
                    We sample rays from regions with high training error and reproject the 3D surface points to different training views to gather image patches. The Ray-Pair RGB Loss is then applied to a set of reference rays and all other rays in these patches. This allows us to regularize the object field using appearance from multiple views simultaneously. <br><br>
                </div>
                <!-- <div class="text-center">
                    <video id="ide" width="100%" playsinline autoplay controls loop muted>
                        <source src="video/ide_animation.mp4" type="video/mp4" />
                    </video>
                </div> -->
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Additional Comparison
                </h3>
                <!-- <div class="video-compare-container">
                    <video class="video" id="musclecar" loop playsinline autoPlay muted src="video/musclecar_mipnerf_ours.mp4" onplay="resizeAndPlay(this)"></video>
                    <canvas height=0 class="videoMerge" id="musclecarMerge"></canvas>
                </div> -->
                <div class="text-center">
                    <img src="./img/comparison_3.png" width="100%">
                    <img src="./img/comparison_4.png" width="100%">
                </div>
			</div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Other Prompts
                </h3>
                Our method also support text prompts using Grounding-DINO, and automatically segmenting everything in the scene using a grid of point prompts:
                <!-- <div class="video-compare-container" style="width: 100%">
                    <video class="video" id="toycar" loop playsinline autoPlay muted src="video/toycar.mp4" onplay="resizeAndPlay(this)"></video>
                    <canvas height=0 class="videoMerge" id="toycarMerge"></canvas>
                </div> -->
                <div class="text-center">
                    <img src="./img/application.png" width="60%">
                </div>
			</div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Dynamic NeRF
                </h3>
                <div class="text-justify">
                    We present a preliminary demonstration on the easy extension of our method to 4D dynamic NeRF representations:
                </div>

                <div style="overflow: hidden;">
                    <video id="dynamic-nerf" width="100%" playsinline controls muted style="margin-top: -5%;">
                        <source src="video/dynamic.mp4" type="video/mp4" />
                    </video>
                </div>

                <!-- <div class="text-justify">
                    We can also control the amounts of specular and diffuse colors, or change the diffuse color without affecting the specular reflections:
                </div>
                
                <table width="100%">
                    <tr>
                        <td align="left" valign="top" width="50%">
                            <video id="v2" width="100%" playsinline autoplay loop muted>
                                <source src="video/car_color2.mp4" type="video/mp4" />
                            </video>
                        </td>
                        <td align="left" valign="top" width="50%">
                            <video id="v3" width="100%" playsinline autoplay loop muted>
                                <source src="video/car_color3.mp4" type="video/mp4" />
                            </video>
                        </td>
                    </tr>
                </table> -->

            </div>
        </div>

            
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@article{liu2023sanerfhq,
    title={SANeRF-HQ: Segment Anything for NeRF in High Quality},
    author={Liu, Yichen and Hu, Benran and Tang, Chi-Keung and Tai, Yu-Wing},
    journal={arXiv preprint arXiv:2312.01531},
    year={2023}
}</textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a> and <a href="https://dorverbin.github.io/refnerf/">Ref-NeRF</a>
                </p>
            </div>
        </div>
    </div>


</body></html>
